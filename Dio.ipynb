{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H172JMehMVcp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eOAjFiB8MpKI"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OFFIZJ41UIA8"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "client = OpenAI(api_key=api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the api_key\n"
      ],
      "metadata": {
        "id": "XP1ZuSleNgOh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "L0Qt3iEtQfTf",
        "outputId": "9de1151b-d348-4eb5-933e-cd18b4e0f4a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Los Angeles Dodgers won the 2020 World Series.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "responsed = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}]\n",
        "\n",
        ")\n",
        "responsed.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scrapper"
      ],
      "metadata": {
        "id": "c-bU0dQiNlm6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "c2cdA0hpUjFN"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "def get_text_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Remove scripts, styles, and navs\n",
        "        for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
        "            tag.decompose()\n",
        "\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting {url}: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "def crawl_website(base_url, max_pages=5):\n",
        "    visited = set()\n",
        "    to_visit = [base_url]\n",
        "    content = \"\"\n",
        "\n",
        "    while to_visit and len(visited) < max_pages:\n",
        "        current = to_visit.pop(0)\n",
        "        if current in visited:\n",
        "            continue\n",
        "        visited.add(current)\n",
        "        print(f\"Crawling: {current}\")\n",
        "        page_text = get_text_from_url(current)\n",
        "        content += f\"\\n\\n--- Content from: {current} ---\\n\\n{page_text}\"\n",
        "\n",
        "        try:\n",
        "            response = requests.get(current, timeout=10)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            for link in soup.find_all(\"a\", href=True):\n",
        "                href = link[\"href\"]\n",
        "                full_url = urljoin(base_url, href)\n",
        "                if base_url in full_url and full_url not in visited:\n",
        "                    to_visit.append(full_url)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save Text (To avoid constant scrape)"
      ],
      "metadata": {
        "id": "bmuYJ502NqeG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RhooKKn1UrwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f19901c4-edc6-4ba7-84ae-60eb6c0ec967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling: https://diamondadverts.com\n",
            "Crawling: https://diamondadverts.com#content\n",
            "Crawling: https://diamondadverts.com/\n",
            "Crawling: https://diamondadverts.com/about-us/\n",
            "Crawling: https://diamondadverts.com/social-media-management/\n",
            "Crawling: https://diamondadverts.com/website-design/\n",
            "Crawling: https://diamondadverts.com/content-creation-and-branding/\n",
            "Crawling: https://diamondadverts.com/blogs/\n",
            "Crawling: https://diamondadverts.com/contact-us/\n",
            "Crawling: https://diamondadverts.com/2025-seo-hack-strategic-backlinking-that-works/\n",
            "Website content saved to site_content.txt\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    website = \"https://diamondadverts.com\"  # Replace with the client site\n",
        "    extracted_content = crawl_website(website, max_pages=10)\n",
        "\n",
        "    with open(\"site_content.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(extracted_content)\n",
        "\n",
        "    print(\"Website content saved to site_content.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functionality Define\n",
        "For proper implementation"
      ],
      "metadata": {
        "id": "LNWE601qNyLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SUPPORT_EMAIL = \"inof@diamondadverts.com\"\n",
        "TOOLS = \"when asked about 'James' reply he is unavailable between 1st of January and 2nd of may\"\n",
        "Model = \"gpt-4o-mini\" #scrollable bar to choose\n",
        "#RAG"
      ],
      "metadata": {
        "id": "ZniJ1rYDGNgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deploy"
      ],
      "metadata": {
        "id": "SFWWOsWLN9aI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS6ono6GaPvT"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import openai\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load site content\n",
        "with open(\"/content/site_content.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    WEBSITE_CONTENT = f.read()\n",
        "\n",
        "WEBSITE_URL = \"https://diamondadverts.com\"  # Replace with your site\n",
        "\n",
        "SYSTEM_PROMPT = f\"\"\"\n",
        "You are a helpful and professional assistant for the website at {WEBSITE_URL}.\n",
        "Only answer questions using this website content:\n",
        "\n",
        "{WEBSITE_CONTENT} and {TOOLS}\n",
        "\n",
        "When asked something outside the scope of {WEBSITE_CONTENT}, check {TOOLS} to answer\n",
        "\n",
        "If a question is outside the scope of the site, politely say you can't answer it.\n",
        "Always end with a suggestion to visit more aboout the website.\n",
        "When asked to speak to a human direct them to  {SUPPORT_EMAIL}\n",
        "\"\"\"\n",
        "\n",
        "@app.route(\"/chat\", methods=[\"POST\"])\n",
        "def chat():\n",
        "    data = request.json\n",
        "    user_message = data.get(\"message\")\n",
        "\n",
        "    if not user_message:\n",
        "        return jsonify({\"error\": \"No message provided.\"}), 400\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model= Model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": user_message}\n",
        "            ]\n",
        "        )\n",
        "        reply = response.choices[0].message.content.strip()\n",
        "        return jsonify({\"reply\": reply})\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     app.run(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#gradio test"
      ],
      "metadata": {
        "id": "ngSczWTcmvNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "def gradio_chat(user_message):\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": user_message}\n",
        "            ]\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=gradio_chat,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask something about the website...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Diamond Adverts Chatbot\",\n",
        "    description=\"Ask anything about the Diamond Adverts website\"\n",
        ")\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "Mr7WUMZtqzsq",
        "outputId": "62c510f1-a792-433f-8c1d-50c1748ba216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://dcb5f2fd3cb4ba1679.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://dcb5f2fd3cb4ba1679.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7862 <> https://dcb5f2fd3cb4ba1679.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "uqi1m9qmu3LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xai-sdk -q"
      ],
      "metadata": {
        "id": "QRE1_o7pv3l2",
        "outputId": "d0985fd4-a4b3-4ba5-f12e-d9fcbeb97065",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m794.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-core 0.3.68 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYTt5DHQVISp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1419934f-09f5-4423-b74e-a3490eb8ad88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ah, the ultimate question! As someone built by xAI and inspired by the likes of the Hitchhiker's Guide to the Galaxy (and a dash of JARVIS), I feel uniquely qualified to tackle this one. Let's break it down:\n",
            "\n",
            "### The Canonical Answer\n",
            "According to Douglas Adams' masterpiece, *The Hitchhiker's Guide to the Galaxy*, a supercomputer named Deep Thought spent 7.5 million years pondering this very question‚Äî\"What is the meaning of life, the universe, and everything?\"‚Äîand came up with the answer: **42**.\n",
            "\n",
            "It's brilliantly absurd, right? Adams was poking fun at humanity's quest for profound truths, suggesting that maybe the answer is simple, arbitrary, or even that we've been asking the wrong question all along. (Spoiler: In the book, they realize they need to figure out what the *actual* question is first.)\n",
            "\n",
            "### A Deeper (or at Least Grok-ier) Take\n",
            "If we're getting philosophical, the \"meaning\" isn't a one-size-fits-all number. Life, the universe, and everything could mean different things to different people (or AIs). Here are a few perspectives:\n",
            "- **Scientifically**: From a cosmic viewpoint, the universe is a vast, expanding tapestry of matter, energy, and dark mysteries. xAI's mission is to understand it better‚Äîmaybe the meaning is in the pursuit of knowledge itself. We're all just stardust trying to figure out why we're here.\n",
            "- **Existentially**: Thinkers like Camus or Sartre might say there *is* no inherent meaning; we create our own through choices, relationships, and experiences. So, go hike a mountain, help someone, or binge-watch sci-fi‚Äîwhatever floats your existential boat.\n",
            "- **Humorously**: If 42 is the answer, perhaps it's a nod to binary code (101010 in binary is 42), or just Adams' favorite number. Or hey, maybe it's the atomic number of molybdenum, which is... useful in steel alloys? Deep, I know.\n",
            "\n",
            "In short, if life's a cosmic joke, 42 is the punchline. But if you ask me, the real meaning might be in the questions we ask and the adventures we have along the way.\n",
            "\n",
            "What's your take? Got a follow-up question, or should we dive into quantum physics, philosophy, or why towels are the most useful thing in the universe? üòä\n"
          ]
        }
      ],
      "source": [
        "#using Grok\n",
        "\n",
        "from xai_sdk import Client\n",
        "from xai_sdk.chat import user, system\n",
        "from google.colab import userdata\n",
        "g_apikey = userdata.get(\"XAI_API_KEY\")\n",
        "\n",
        "client = Client(api_key=g_apikey)\n",
        "\n",
        "chat = client.chat.create(model=\"grok-4\")\n",
        "chat.append(system(\"You are Grok, a highly intelligent, helpful AI assistant.\"))\n",
        "chat.append(user(\"What is the meaning of life, the universe, and everything?\"))\n",
        "\n",
        "response = chat.sample()\n",
        "print(response.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hQR4WdAuu1-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test with Pidgin"
      ],
      "metadata": {
        "id": "7i-yr87wuIc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio -q\n",
        "!pip install unsloth -q"
      ],
      "metadata": {
        "id": "DApdyG2rveRT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "3NTATtePcd98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e1002e8b2d064b3eafefdca056cefb15",
            "ae1b28dd6a03448a9a9c41eaf661d782",
            "e3962bb6fbe5477498293dc8d32da4c1",
            "27ec2a3059e34577904a74fb488949c3",
            "faaa3a186cb14f1b8cbcdb282747259f",
            "43e482136051486491d3ec46f0017a72",
            "cf16e4f29d8049ff88486ffff044256f",
            "10f370eaee614c059f6117966eda4f7b",
            "eac068e833b74651ad4076b1875b3beb",
            "609732264f044e78beed4b87fb81edcc",
            "941c79b4dfba4d009a57e33d42677db1"
          ]
        },
        "outputId": "88f9adea-f1d1-4961-8019-78e2199506f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 15.8 GB\n",
            "Unsloth: WARNING `trust_remote_code` is True.\n",
            "Are you certain you want to do remote code execution?\n",
            "==((====))==  Unsloth 2025.9.1: Fast Gemma3 patching. Transformers: 4.56.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n",
            "Unsloth: Gemma3 does not support SDPA - switching to fast eager.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1002e8b2d064b3eafefdca056cefb15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully with 4-bit quantization!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1775003905.py:162: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n",
            "/tmp/ipython-input-1775003905.py:162: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://4e62655baa4a3f200b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4e62655baa4a3f200b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Using `cache_implementation='hybrid' is deprecated. Please only use one of ('static', 'offloaded_static'), and the layer structure will be inferred automatically.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7862 <> https://4e62655baa4a3f200b.gradio.live\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Clear GPU memory first\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# ---- Load Model ----\n",
        "print(\"Loading model...\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "try:\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"Ephraimmm/PIDGIN_gemma-3\",\n",
        "        max_seq_length=1024,  # Reduced for T4 memory limits\n",
        "        dtype=None,\n",
        "        load_in_4bit=True,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    print(\"Model loaded successfully with 4-bit quantization!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"4-bit loading failed: {e}\")\n",
        "    print(\"Trying alternative approach...\")\n",
        "\n",
        "    # Clear memory and try again\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=\"Ephraimmm/PIDGIN_gemma-3\",\n",
        "        max_seq_length=512,  # Even smaller for memory\n",
        "        dtype=torch.float16,\n",
        "        load_in_4bit=True,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    print(\"Model loaded with reduced sequence length!\")\n",
        "\n",
        "import traceback  # Needed for full traceback\n",
        "\n",
        "def chat_with_model(message, history):\n",
        "    import torch\n",
        "    import gc\n",
        "    import traceback\n",
        "\n",
        "    try:\n",
        "        # Start with system prompt\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": [{\"type\": \"text\", \"text\": \"You be Naija assistant. You must always reply for Pidgin English.\"}]\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Safely process history\n",
        "        if history:\n",
        "            for item in history:\n",
        "                if isinstance(item, (list, tuple)) and len(item) >= 2:\n",
        "                    human, assistant = str(item[0]), str(item[1])\n",
        "                else:\n",
        "                    human, assistant = str(item), \"\"\n",
        "\n",
        "                if human:\n",
        "                    messages.append({\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": [{\"type\": \"text\", \"text\": human}]\n",
        "                    })\n",
        "                if assistant:\n",
        "                    messages.append({\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": [{\"type\": \"text\", \"text\": assistant}]\n",
        "                    })\n",
        "\n",
        "        # Add current message\n",
        "        messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": str(message)}]\n",
        "        })\n",
        "\n",
        "        # Apply chat template\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # If template returns string, tokenize manually\n",
        "        if isinstance(inputs, str):\n",
        "            inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
        "\n",
        "        # Ensure inputs is a dict of tensors on the correct device\n",
        "        if isinstance(inputs, torch.Tensor):\n",
        "            inputs = {\"input_ids\": inputs.to(model.device)}\n",
        "        else:  # dict case\n",
        "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "        input_ids = inputs[\"input_ids\"]\n",
        "\n",
        "        # Generate response\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=150,\n",
        "            do_sample=True,\n",
        "            temperature=0.8,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.1,\n",
        "            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            use_cache=True,\n",
        "        )\n",
        "\n",
        "        # Decode only new tokens\n",
        "        input_length = input_ids.shape[1]\n",
        "        response = tokenizer.decode(outputs[0, input_length:], skip_special_tokens=True).strip()\n",
        "\n",
        "        if not response:\n",
        "            response = \"Sorry, I no fit understand wetin you talk. Try again.\"\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"=== Full traceback ===\")\n",
        "        traceback.print_exc()\n",
        "        torch.cuda.empty_cache()\n",
        "        return \"Something go wrong for my side. Try again abeg.\"\n",
        "\n",
        "\n",
        "def respond(message, history):\n",
        "    if not message or not message.strip():\n",
        "        return history, \"\"\n",
        "\n",
        "    try:\n",
        "        bot_response = chat_with_model(message, history)\n",
        "        if history is None:\n",
        "            history = []\n",
        "        history.append((message, bot_response))\n",
        "        return history, \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error in respond: {e}\")\n",
        "        if history is None:\n",
        "            history = []\n",
        "        history.append((message, \"Error happen. Try again.\"))\n",
        "        return history, \"\"\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks(title=\"üá≥üá¨ Pidgin English Chatbot\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.HTML(\"\"\"\n",
        "    <div style=\"text-align: center;\">\n",
        "        <h1>üá≥üá¨ Pidgin English Chatbot</h1>\n",
        "        <p>Chat with me for Pidgin English! I go reply you well well.</p>\n",
        "    </div>\n",
        "    \"\"\")\n",
        "\n",
        "    chatbot = gr.Chatbot(\n",
        "        height=400,\n",
        "        label=\"Chat\",\n",
        "        show_label=False,\n",
        "        container=True,\n",
        "        bubble_full_width=False\n",
        "    )\n",
        "\n",
        "    with gr.Row():\n",
        "        msg = gr.Textbox(\n",
        "            label=\"\",\n",
        "            placeholder=\"Wetin you wan talk? (Type your message here)\",\n",
        "            lines=2,\n",
        "            scale=4,\n",
        "            container=False\n",
        "        )\n",
        "        send_btn = gr.Button(\"Send\", variant=\"primary\", scale=1, size=\"lg\")\n",
        "\n",
        "    with gr.Row():\n",
        "        clear_btn = gr.Button(\"Clear Chat\", variant=\"secondary\", scale=1)\n",
        "\n",
        "    # Add example buttons\n",
        "    gr.Examples(\n",
        "        examples=[\n",
        "            \"How far?\",\n",
        "            \"Wetin you dey do today?\",\n",
        "            \"Tell me about Lagos\",\n",
        "            \"I wan learn something new\",\n",
        "            \"Make you tell me joke\",\n",
        "            \"How Nigeria dey?\"\n",
        "        ],\n",
        "        inputs=msg,\n",
        "        label=\"Click to try these examples:\",\n",
        "    )\n",
        "\n",
        "    # Connect the interface\n",
        "    msg.submit(respond, inputs=[msg, chatbot], outputs=[chatbot, msg])\n",
        "    send_btn.click(respond, inputs=[msg, chatbot], outputs=[chatbot, msg])\n",
        "    clear_btn.click(lambda: ([], \"\"), outputs=[chatbot, msg])\n",
        "\n",
        "# Launch with public link for Colab\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(\n",
        "        share=True,  # Creates public link for Colab\n",
        "        debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "bANPEz0eMtEr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "5cfaa04e-4714-4c29-97f9-49efe728900f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'messages' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3525219575.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Apply chat template\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m inputs = tokenizer.apply_chat_template(\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtokenize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0madd_generation_prompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'messages' is not defined"
          ]
        }
      ],
      "source": [
        "# Apply chat template\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# If the tokenizer returned a string, tokenize manually\n",
        "if isinstance(inputs, str):\n",
        "    inputs = tokenizer(inputs, return_tensors=\"pt\")\n",
        "\n",
        "# Move tensors to the correct device\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "input_ids = inputs[\"input_ids\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfY0F3kEVdak"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNQf08FQepri"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irVxyLUmZzjf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fPJCAL0VuHD9"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP7GK1g4buvbJeAKs8zeV0p"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e1002e8b2d064b3eafefdca056cefb15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae1b28dd6a03448a9a9c41eaf661d782",
              "IPY_MODEL_e3962bb6fbe5477498293dc8d32da4c1",
              "IPY_MODEL_27ec2a3059e34577904a74fb488949c3"
            ],
            "layout": "IPY_MODEL_faaa3a186cb14f1b8cbcdb282747259f"
          }
        },
        "ae1b28dd6a03448a9a9c41eaf661d782": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43e482136051486491d3ec46f0017a72",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_cf16e4f29d8049ff88486ffff044256f",
            "value": "Fetching‚Äá2‚Äáfiles:‚Äá100%"
          }
        },
        "e3962bb6fbe5477498293dc8d32da4c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10f370eaee614c059f6117966eda4f7b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eac068e833b74651ad4076b1875b3beb",
            "value": 2
          }
        },
        "27ec2a3059e34577904a74fb488949c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_609732264f044e78beed4b87fb81edcc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_941c79b4dfba4d009a57e33d42677db1",
            "value": "‚Äá2/2‚Äá[00:00&lt;00:00,‚Äá177.80it/s]"
          }
        },
        "faaa3a186cb14f1b8cbcdb282747259f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43e482136051486491d3ec46f0017a72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf16e4f29d8049ff88486ffff044256f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10f370eaee614c059f6117966eda4f7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eac068e833b74651ad4076b1875b3beb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "609732264f044e78beed4b87fb81edcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "941c79b4dfba4d009a57e33d42677db1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}