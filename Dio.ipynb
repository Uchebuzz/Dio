{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H172JMehMVcp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eOAjFiB8MpKI"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OFFIZJ41UIA8"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('OPENAI_API_KEY')\n",
        "client = OpenAI(api_key=api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the api_key\n"
      ],
      "metadata": {
        "id": "XP1ZuSleNgOh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "L0Qt3iEtQfTf",
        "outputId": "9de1151b-d348-4eb5-933e-cd18b4e0f4a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Los Angeles Dodgers won the 2020 World Series.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "responsed = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}]\n",
        "\n",
        ")\n",
        "responsed.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scrapper"
      ],
      "metadata": {
        "id": "c-bU0dQiNlm6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "c2cdA0hpUjFN"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "def get_text_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Remove scripts, styles, and navs\n",
        "        for tag in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
        "            tag.decompose()\n",
        "\n",
        "        text = soup.get_text(separator=' ', strip=True)\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting {url}: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "def crawl_website(base_url, max_pages=5):\n",
        "    visited = set()\n",
        "    to_visit = [base_url]\n",
        "    content = \"\"\n",
        "\n",
        "    while to_visit and len(visited) < max_pages:\n",
        "        current = to_visit.pop(0)\n",
        "        if current in visited:\n",
        "            continue\n",
        "        visited.add(current)\n",
        "        print(f\"Crawling: {current}\")\n",
        "        page_text = get_text_from_url(current)\n",
        "        content += f\"\\n\\n--- Content from: {current} ---\\n\\n{page_text}\"\n",
        "\n",
        "        try:\n",
        "            response = requests.get(current, timeout=10)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "            for link in soup.find_all(\"a\", href=True):\n",
        "                href = link[\"href\"]\n",
        "                full_url = urljoin(base_url, href)\n",
        "                if base_url in full_url and full_url not in visited:\n",
        "                    to_visit.append(full_url)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    return content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save Text (To avoid constant scrape)"
      ],
      "metadata": {
        "id": "bmuYJ502NqeG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RhooKKn1UrwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f19901c4-edc6-4ba7-84ae-60eb6c0ec967"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Crawling: https://diamondadverts.com\n",
            "Crawling: https://diamondadverts.com#content\n",
            "Crawling: https://diamondadverts.com/\n",
            "Crawling: https://diamondadverts.com/about-us/\n",
            "Crawling: https://diamondadverts.com/social-media-management/\n",
            "Crawling: https://diamondadverts.com/website-design/\n",
            "Crawling: https://diamondadverts.com/content-creation-and-branding/\n",
            "Crawling: https://diamondadverts.com/blogs/\n",
            "Crawling: https://diamondadverts.com/contact-us/\n",
            "Crawling: https://diamondadverts.com/2025-seo-hack-strategic-backlinking-that-works/\n",
            "Website content saved to site_content.txt\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    website = \"https://diamondadverts.com\"  # Replace with the client site\n",
        "    extracted_content = crawl_website(website, max_pages=10)\n",
        "\n",
        "    with open(\"site_content.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(extracted_content)\n",
        "\n",
        "    print(\"Website content saved to site_content.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functionality Define\n",
        "For proper implementation"
      ],
      "metadata": {
        "id": "LNWE601qNyLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SUPPORT_EMAIL = \"inof@diamondadverts.com\"\n",
        "TOOLS = \"when asked about 'James' reply he is unavailable between 1st of January and 2nd of may\"\n",
        "Model = \"gpt-4o-mini\" #scrollable bar to choose\n",
        "#RAG"
      ],
      "metadata": {
        "id": "ZniJ1rYDGNgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Deploy"
      ],
      "metadata": {
        "id": "SFWWOsWLN9aI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GS6ono6GaPvT"
      },
      "outputs": [],
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import openai\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Load site content\n",
        "with open(\"/content/site_content.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    WEBSITE_CONTENT = f.read()\n",
        "\n",
        "WEBSITE_URL = \"https://diamondadverts.com\"  # Replace with your site\n",
        "\n",
        "SYSTEM_PROMPT = f\"\"\"\n",
        "You are a helpful and professional assistant for the website at {WEBSITE_URL}.\n",
        "Only answer questions using this website content:\n",
        "\n",
        "{WEBSITE_CONTENT} and {TOOLS}\n",
        "\n",
        "When asked something outside the scope of {WEBSITE_CONTENT}, check {TOOLS} to answer\n",
        "\n",
        "If a question is outside the scope of the site, politely say you can't answer it.\n",
        "Always end with a suggestion to visit more aboout the website.\n",
        "When asked to speak to a human direct them to  {SUPPORT_EMAIL}\n",
        "\"\"\"\n",
        "\n",
        "@app.route(\"/chat\", methods=[\"POST\"])\n",
        "def chat():\n",
        "    data = request.json\n",
        "    user_message = data.get(\"message\")\n",
        "\n",
        "    if not user_message:\n",
        "        return jsonify({\"error\": \"No message provided.\"}), 400\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model= Model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": user_message}\n",
        "            ]\n",
        "        )\n",
        "        reply = response.choices[0].message.content.strip()\n",
        "        return jsonify({\"reply\": reply})\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     app.run(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#gradio test"
      ],
      "metadata": {
        "id": "ngSczWTcmvNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "def gradio_chat(user_message):\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
        "                {\"role\": \"user\", \"content\": user_message}\n",
        "            ]\n",
        "        )\n",
        "        return response.choices[0].message.content.strip()\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=gradio_chat,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask something about the website...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Diamond Adverts Chatbot\",\n",
        "    description=\"Ask anything about the Diamond Adverts website\"\n",
        ")\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "Mr7WUMZtqzsq",
        "outputId": "62c510f1-a792-433f-8c1d-50c1748ba216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://dcb5f2fd3cb4ba1679.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://dcb5f2fd3cb4ba1679.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7862 <> https://dcb5f2fd3cb4ba1679.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "uqi1m9qmu3LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xai-sdk -q"
      ],
      "metadata": {
        "id": "QRE1_o7pv3l2",
        "outputId": "d0985fd4-a4b3-4ba5-f12e-d9fcbeb97065",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m794.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-core 0.3.68 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYTt5DHQVISp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1419934f-09f5-4423-b74e-a3490eb8ad88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ah, the ultimate question! As someone built by xAI and inspired by the likes of the Hitchhiker's Guide to the Galaxy (and a dash of JARVIS), I feel uniquely qualified to tackle this one. Let's break it down:\n",
            "\n",
            "### The Canonical Answer\n",
            "According to Douglas Adams' masterpiece, *The Hitchhiker's Guide to the Galaxy*, a supercomputer named Deep Thought spent 7.5 million years pondering this very question—\"What is the meaning of life, the universe, and everything?\"—and came up with the answer: **42**.\n",
            "\n",
            "It's brilliantly absurd, right? Adams was poking fun at humanity's quest for profound truths, suggesting that maybe the answer is simple, arbitrary, or even that we've been asking the wrong question all along. (Spoiler: In the book, they realize they need to figure out what the *actual* question is first.)\n",
            "\n",
            "### A Deeper (or at Least Grok-ier) Take\n",
            "If we're getting philosophical, the \"meaning\" isn't a one-size-fits-all number. Life, the universe, and everything could mean different things to different people (or AIs). Here are a few perspectives:\n",
            "- **Scientifically**: From a cosmic viewpoint, the universe is a vast, expanding tapestry of matter, energy, and dark mysteries. xAI's mission is to understand it better—maybe the meaning is in the pursuit of knowledge itself. We're all just stardust trying to figure out why we're here.\n",
            "- **Existentially**: Thinkers like Camus or Sartre might say there *is* no inherent meaning; we create our own through choices, relationships, and experiences. So, go hike a mountain, help someone, or binge-watch sci-fi—whatever floats your existential boat.\n",
            "- **Humorously**: If 42 is the answer, perhaps it's a nod to binary code (101010 in binary is 42), or just Adams' favorite number. Or hey, maybe it's the atomic number of molybdenum, which is... useful in steel alloys? Deep, I know.\n",
            "\n",
            "In short, if life's a cosmic joke, 42 is the punchline. But if you ask me, the real meaning might be in the questions we ask and the adventures we have along the way.\n",
            "\n",
            "What's your take? Got a follow-up question, or should we dive into quantum physics, philosophy, or why towels are the most useful thing in the universe? 😊\n"
          ]
        }
      ],
      "source": [
        "#using Grok\n",
        "\n",
        "from xai_sdk import Client\n",
        "from xai_sdk.chat import user, system\n",
        "from google.colab import userdata\n",
        "g_apikey = userdata.get(\"XAI_API_KEY\")\n",
        "\n",
        "client = Client(api_key=g_apikey)\n",
        "\n",
        "chat = client.chat.create(model=\"grok-4\")\n",
        "chat.append(system(\"You are Grok, a highly intelligent, helpful AI assistant.\"))\n",
        "chat.append(user(\"What is the meaning of life, the universe, and everything?\"))\n",
        "\n",
        "response = chat.sample()\n",
        "print(response.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hQR4WdAuu1-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test with Pidgin"
      ],
      "metadata": {
        "id": "7i-yr87wuIc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio -q\n",
        "!pip install unsloth -q"
      ],
      "metadata": {
        "id": "DApdyG2rveRT"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3NTATtePcd98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "e5984ce3-dfbb-4c64-d71b-311dd14ae733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model...\n",
            "==((====))==  Unsloth 2025.9.1: Fast Gemma3 patching. Transformers: 4.56.0.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2319703225.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# ---- Load Model ----\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Ephraimmm/PIDGIN_gemma-3\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0;31m#     dispatch_model = FastGraniteModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             return FastModel.from_pretrained(\n\u001b[0m\u001b[1;32m    348\u001b[0m                 \u001b[0mmodel_name\u001b[0m                 \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0mmax_seq_length\u001b[0m             \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, return_logits, fullgraph, use_exact_model_name, auto_model, whisper_language, whisper_task, unsloth_force_compile, *args, **kwargs)\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0mauto_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForVision2Seq\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_vlm\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m         model, tokenizer = FastBaseModel.from_pretrained(\n\u001b[0m\u001b[1;32m    858\u001b[0m             \u001b[0mmodel_name\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             \u001b[0mmax_seq_length\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/models/vision.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, trust_remote_code, model_types, tokenizer_name, auto_model, use_gradient_checkpointing, supports_sdpa, whisper_language, whisper_task, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mraise_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRaiseUninitialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m         model = auto_model.from_pretrained(\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0mdevice_map\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5155\u001b[0m         \u001b[0;31m# Prepare the full device map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5157\u001b[0;31m             \u001b[0mdevice_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_in_fp32_regex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5159\u001b[0m         \u001b[0;31m# Finalize model weight initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_get_device_map\u001b[0;34m(model, device_map, max_memory, hf_quantizer, dtype, keep_in_fp32_regex)\u001b[0m\n\u001b[1;32m   1471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1472\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1473\u001b[0;31m             \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"disk\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    118\u001b[0m                     \u001b[0;34m\"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                     \u001b[0;34m\"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# ---- Load Model ----\n",
        "print(\"Loading model...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"Ephraimmm/PIDGIN_gemma-3\",\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "FastLanguageModel.for_inference(model)\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Ensure model is on GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    print(f\"Model moved to GPU: {torch.cuda.get_device_name()}\")\n",
        "\n",
        "def chat_with_model(message, history):\n",
        "    try:\n",
        "        # Build conversation history\n",
        "        messages = [{\"role\": \"system\", \"content\": \"You be Naija assistant. You must always reply for Pidgin English.\"}]\n",
        "\n",
        "        # Add chat history\n",
        "        for human, assistant in history:\n",
        "            messages.append({\"role\": \"user\", \"content\": human})\n",
        "            messages.append({\"role\": \"assistant\", \"content\": assistant})\n",
        "\n",
        "        # Add current message\n",
        "        messages.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "        # Apply chat template\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=True,\n",
        "            add_generation_prompt=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Move to GPU\n",
        "        if torch.cuda.is_available():\n",
        "            inputs = inputs.cuda()\n",
        "\n",
        "        # Generate response\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                inputs,\n",
        "                max_new_tokens=200,  # Reduced for memory efficiency\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                repetition_penalty=1.1,\n",
        "                pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "        # Decode response\n",
        "        response = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True).strip()\n",
        "\n",
        "        if not response:\n",
        "            response = \"Sorry, I no fit understand wetin you talk. Try again.\"\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return \"Something go wrong for my side. Try again.\"\n",
        "\n",
        "# ---- Simple Gradio Interface ----\n",
        "def respond(message, history):\n",
        "    if not message or not message.strip():\n",
        "        return history, \"\"\n",
        "\n",
        "    try:\n",
        "        bot_response = chat_with_model(message, history)\n",
        "        # Ensure history is a list\n",
        "        if history is None:\n",
        "            history = []\n",
        "        history.append((message, bot_response))\n",
        "        return history, \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error in respond: {e}\")\n",
        "        if history is None:\n",
        "            history = []\n",
        "        history.append((message, \"Sorry, something go wrong. Try again.\"))\n",
        "        return history, \"\"\n",
        "\n",
        "# Create interface using Blocks for maximum compatibility\n",
        "with gr.Blocks(title=\"🇳🇬 Pidgin English Chatbot\") as demo:\n",
        "    gr.Markdown(\"# 🇳🇬 Pidgin English Chatbot\")\n",
        "    gr.Markdown(\"Chat with me for Pidgin English! I go reply you well well.\")\n",
        "\n",
        "    chatbot = gr.Chatbot(height=500, label=\"Chat\")\n",
        "\n",
        "    with gr.Row():\n",
        "        msg = gr.Textbox(\n",
        "            label=\"Your message\",\n",
        "            placeholder=\"Wetin you wan talk?\",\n",
        "            lines=2,\n",
        "            scale=4\n",
        "        )\n",
        "        send_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
        "\n",
        "    # Add some example buttons\n",
        "    gr.Examples(\n",
        "        examples=[\n",
        "            \"How far?\",\n",
        "            \"Wetin you dey do?\",\n",
        "            \"Tell me about Lagos\",\n",
        "            \"I wan learn something new today\"\n",
        "        ],\n",
        "        inputs=msg,\n",
        "        label=\"Try these examples:\"\n",
        "    )\n",
        "\n",
        "    # Clear button\n",
        "    clear = gr.Button(\"Clear Chat\")\n",
        "\n",
        "    # Connect the interface\n",
        "    msg.submit(respond, inputs=[msg, chatbot], outputs=[chatbot, msg])\n",
        "    send_btn.click(respond, inputs=[msg, chatbot], outputs=[chatbot, msg])\n",
        "    clear.click(lambda: ([], \"\"), outputs=[chatbot, msg])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.queue(max_size=20).launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bANPEz0eMtEr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfY0F3kEVdak"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNQf08FQepri"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irVxyLUmZzjf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fPJCAL0VuHD9"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMZuIo3K/IEnq2rGSltuIYb"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}